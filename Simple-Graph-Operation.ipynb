{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbaseconda0d655350af4240fca330c725287add23",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A High-Level Introduction to Graph Convolutional Networks\n",
    "\n",
    "> [How to do Deep Learning on Graphs with Graph Convolutional Networks Part 1: A High-Level Introduction to Graph Convolutional Networks]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Adjacency matrix: \n [[0. 1. 0. 0.]\n [0. 0. 1. 1.]\n [0. 1. 0. 0.]\n [1. 0. 1. 0.]]\nFeature matrix: \n [[ 0.  0.]\n [ 1. -1.]\n [ 2. -2.]\n [ 3. -3.]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# simple directed graph\n",
    "# 0 -> 1\n",
    "# 1 -> 2, 1 -> 3\n",
    "# 2 -> 1\n",
    "# 3 -> 0, 3 -> 2\n",
    "\n",
    "# adjacency matrix\n",
    "A = np.array([\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 1, 0, 0],\n",
    "    [1, 0, 1, 0]\n",
    "], dtype=float)\n",
    "print('Adjacency matrix: \\n',A)\n",
    "\n",
    "# feature matrix\n",
    "X = np.array([\n",
    "    [i, -i] for i in range(A.shape[0])\n",
    "], dtype=float)\n",
    "print('Feature matrix: \\n', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## propagation rule \n",
    "$$f(X,A) = AX$$\n",
    "- The representation of each node (each row) is now a sum of its neighbors features!\n",
    "- Note: a node n is a neighbor of node v if there exists an edge from v to n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[ 1. -1.]\n [ 5. -5.]\n [ 1. -1.]\n [ 2. -2.]]\n"
    }
   ],
   "source": [
    "print(np.matmul(A, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problems:\n",
    "1. The aggregated representation of a node does not include its own features! Only nodes that has a self-loop will include their own features in the aggregate.\n",
    "2. Nodes with large degrees will have large values in their feature representation otherwise the opposite. This causes vanishing or exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solitions:\n",
    "1. Adding Self-Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[ 1. -1.]\n [ 6. -6.]\n [ 3. -3.]\n [ 5. -5.]]\n"
    }
   ],
   "source": [
    "I = np.eye(A.shape[0])\n",
    "A_s = A + I\n",
    "print(np.matmul(A_s, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Normalizing the Feature Representations\n",
    "- Transform the adjacency matrix A by multiplying it with the inverse degree matrix D\n",
    "$$ f(X,A) = D^{-1}AX $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Degree matrix: \n [[1. 0. 0. 0.]\n [0. 2. 0. 0.]\n [0. 0. 2. 0.]\n [0. 0. 0. 1.]]\ninv(D)A: \n [[0.  1.  0.  0. ]\n [0.  0.  0.5 0.5]\n [0.  0.5 0.  0. ]\n [1.  0.  1.  0. ]]\ninv(D)AX: \n [[ 1.  -1. ]\n [ 2.5 -2.5]\n [ 0.5 -0.5]\n [ 2.  -2. ]]\n"
    }
   ],
   "source": [
    "D = np.array(np.sum(A, axis=0))\n",
    "D = np.array(np.diag(D))\n",
    "print('Degree matrix: \\n', D)\n",
    "print('inv(D)A: \\n', np.matmul(np.linalg.inv(D), A))\n",
    "print('inv(D)AX: \\n', np.matmul(np.matmul(np.linalg.inv(D), A), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}